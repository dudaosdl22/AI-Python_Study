# AI스터디 파트1 [파이토치 기초]
#우왕 신기하다ㅏ
## 개념정리1
### 1. 파이토치(PyTorch)
+ torch: 텐서를 생성하는 라이브러리

+ torch.autograd: 자동미분 기능을 제공하는 라이브러리

+ torch.nn: 신경망을 생성하는 라이브러리

+ torch.multiprocessing: 병럴처리 기능을 제공하는 라이브러리
+ torch.utils: 데이터 조작 등 유틸리티 기능 제공

+ torch.legacy(./nn/.optim): Torch로부터 포팅해온 코드

+ torch.onnx: ONNX(Open Neural Network Exchange): 서로 다른 프레임워크 간의 모델을 공유할 때 사용

### 2. 텐서(Tensors)
+ 넘파이(NumPy)의 ndarray와 유사
+ GPU를 사용한 연산 가속도 가능

##실습
+ 초기화 되지않은 행렬
~~~
x = torch.empty(4, 2) #4x2행렬 생성, 초기화 되지 않아 값 존재
print(x)

 # 결과
 tensor([[2.1707e-18, 7.0952e+22],
        [1.7748e+28, 1.8176e+31],
        [7.2708e+31, 5.0778e+31],
        [3.2608e-12, 1.7728e+28]])
~~~

+ 무작위로 초기화된 행렬
~~~
x = torch.rand(4,2) #랜덤으로 4x2 행렬 생성
print(x)

 # 결과
 tensor([[0.4462, 0.3462],
        [0.1912, 0.4335],
        [0.3261, 0.0525],
        [0.1875, 0.1372]])
~~~

+ 데이터타입이 long, 0으로 채워진 텐서
~~~
x = torch.zeros(4, 2, dtype=torch.long) #데이터타입이 long인 0으로 이루어진 4x2 행렬 생성
print(x)

 # 결과
 tensor([[0, 0],
        [0, 0],
        [0, 0],
        [0, 0]])
~~~

~~~
x = torch.tensor([3, 2.3]) #직접 텐서 생성
print(x)

 # 결과
 tensor([3.0000, 2.3000])
~~~

~~~
x = x.new_ones(2, 4, dtype=torch.double) #double타입 1로 채워진 2x4 행렬 생성
print(x)

 # 결과
 tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]], dtype=torch.float64)
~~~

~~~
x = torch.randn_like(x, dtype=torch.float) #행렬x의 모양(2x4)를 그대로 따와(like) float타입 랜덤값 삽입
print(x)

 # 결과 
tensor([[-0.7258,  0.3146, -1.3818,  0.0024],
        [-1.4790,  0.4651, -0.7421,  0.6227]])
~~~

+ 텐서의 크기
~~~
print(x.size())

 # 결과
 torch.Size([2, 4])
~~~

+ 텐서의 연산
~~~
두 텐서 x, y가 있을때, 

1. 덧셈
1.1. (x + y)

1.2. (torch.add(x, y))

1.3. result = torch.empty(2, 4)
     torch.add(x, y, out=result) #x와 y의 합을 result로 
     print(result)

1.4. y.add_(x) #y값에 x값이 포함된 형태로 연산 진행(in place 방식)
     print(y)



2. 뺄셈
2.1. (x - y)

2.2. (torch.sub(x,y))

2.3. (x.sub(y)) 



3. 곱셈
3.1. (x * y)

3.2. (torch.mul(x,y))

3.3. (x.mul(y))



4. 나눗셈
4.1. (x / y)

4.2. (torch.div(x,y))

4.3. (x.div(y)) #모두 동일한 결과
 


5. 내적
print(torch.mm(x, y)) #행렬 곱
~~~

+ 텐서의 조작[인덱싱] => 넘파이처럼 인덱싱 사용가능
~~~
텐서X
tensor([[1, 3],
        [5, 7]]) 일때,

print(x[:, 1]) #두번째 컬럼에 대해서만 출력(모든 행에 첫번째 열이므로)
 
 # 결과
 tensor([3, 7])
~~~

+ 텐서의 조작[view] => 텐서의 크기나 모양 변경
~~~
x = torch.randn(4, 5)
y = x.view(20)
z = x.view(5, -1)

print(x)
print(y)
print(z)
print("--------------------------------------------")
print(x.size())
print(y.size())
print(z.size())

 # 결과
 tensor([[ 0.8223,  1.0609,  0.2464,  0.1893,  0.1195],
        [-0.0171, -0.1404, -0.6623,  0.8181, -0.6462],
        [ 0.9821,  0.8277,  0.2429,  0.7789,  0.0673],
        [ 1.2188,  1.1324, -0.8076,  1.2628,  1.4735]])
tensor([ 0.8223,  1.0609,  0.2464,  0.1893,  0.1195, -0.0171, -0.1404, -0.6623,
         0.8181, -0.6462,  0.9821,  0.8277,  0.2429,  0.7789,  0.0673,  1.2188,
         1.1324, -0.8076,  1.2628,  1.4735])
tensor([[ 0.8223,  1.0609,  0.2464,  0.1893],
        [ 0.1195, -0.0171, -0.1404, -0.6623],
        [ 0.8181, -0.6462,  0.9821,  0.8277],
        [ 0.2429,  0.7789,  0.0673,  1.2188],
        [ 1.1324, -0.8076,  1.2628,  1.4735]])
--------------------------------------------
torch.Size([4, 5])
torch.Size([20])
torch.Size([5, 4])
~~~

+ 텐서의 조작[item] => 텐서의 값이 단 하나라도 존재하면 숫자값을 얻을 수 있음.
  
주의) 스칼라값이 하나만 존재해야 함 !!
~~~
x = torch.randn(1)
print(x)
print(x.item()) #item => -0.0520의 실제값 출력
print(x.dtype)

 # 결과
 tensor([-0.0520])
-0.051953621208667755
torch.float32
~~~

+ 텐서의 조작[squeeze] => 차원을 축소
~~~
tensor = torch.rand(1, 3, 3)
print(tensor)
tensor.shape

 # 결과1 
tensor([[[0.7937, 0.3497, 0.9173],
         [0.3092, 0.8956, 0.7378],
         [0.2639, 0.6195, 0.5072]]])
torch.Size([1, 3, 3])


결과1의 텐서를
t = tensor.squeeze()

print(t)
print(t.shape)
를 통해

 # 결과2
tensor([[0.2178, 0.2915, 0.3858],
        [0.9318, 0.8324, 0.0679],
        [0.3122, 0.6248, 0.9754]])
torch.Size([3, 3])

로 축소
~~~

+ 텐서의 조작[unsqueeze] => 차원을 증가
~~~
tensor = torch.rand(1,3,3)
print(tensor)
print(tensor.shape)

 # 결과1
 tensor([[[0.3571, 0.0282, 0.0419],
         [0.0902, 0.3964, 0.7608],
         [0.2198, 0.4584, 0.8408]]])
torch.Size([1, 3, 3])

결과1의 텐서를
t = tensor.unsqueeze(dim=0) #dim=0이므로 첫번째 차원 증가
print(t)
print(t.shape)
를 통해

 # 결과2
 tensor([[[[0.3571, 0.0282, 0.0419],
          [0.0902, 0.3964, 0.7608],
          [0.2198, 0.4584, 0.8408]]]])
torch.Size([1, 1, 3, 3])

로 증가시킴
~~~

+ 텐서의 조작[stack] => 텐서 간 결합
~~~
x = torch.FloatTensor([1,4])
y = torch.FloatTensor([2,5])
z = torch.FloatTensor([3,6])

print(torch.stack([x, y, z])) #텐서들이 결합되어서 나옴

 # 결과

tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
~~~

+ 텐서의 조작[cat] => 텐서를 결합하는 메소드(concatenate),  넘파이의 stack과 유사하지만, 쌓을 dim이 존재해야함
~~~
a = torch.randn(1, 1, 3, 3)
b = torch.randn(1, 1, 3, 3)
c = torch.cat((a,b), dim=0)

print(c)
print(c.size())

 # 결과
 tensor([[[[-0.3459, -2.1832,  0.0639],
          [ 0.4443, -0.4942, -0.2480],
          [ 0.3140,  0.4265, -1.3259]]],


        [[[ 0.7484,  0.1739, -0.6786],
          [-0.6681, -0.4214, -1.8598],
          [ 1.2128,  0.0104, -0.5977]]]])
torch.Size([2, 1, 3, 3])
~~~

+ 텐서의 조작[chunk] => 텐서를 여러 개로 나눌 때 사용(몇개의 텐서로 나눌 것이냐)
~~~
tensor = torch.rand(3, 6)
t1, t2, t3, = torch.chunk(tensor, 3, dim=1) #dim=0은 행방향, dim=1은 열방향

print(tensor)
print(t1)
print(t2)
print(t3)

 # 결과
 tensor([[0.7009, 0.8951, 0.6954, 0.9761, 0.1431, 0.7700],
        [0.7658, 0.3744, 0.9625, 0.4063, 0.8316, 0.5190],
        [0.0217, 0.4745, 0.2214, 0.7771, 0.3278, 0.7269]])
tensor([[0.7009, 0.8951],
        [0.7658, 0.3744],
        [0.0217, 0.4745]])
tensor([[0.6954, 0.9761],
        [0.9625, 0.4063],
        [0.2214, 0.7771]])
tensor([[0.1431, 0.7700],
        [0.8316, 0.5190],
        [0.3278, 0.7269]])
~~~

+ 텐서의 조작[split] => chunck와 동일한 기능이지만 조금 다름 (하나의 텐서당 크기가 얼마이냐)
~~~
tensor = torch.rand(3, 6)
t1, t2 = torch.split(tensor, 3, dim=1)

print(tensor)
print(t1)
print(t2)

 # 결과
 tensor([[0.2205, 0.0761, 0.4101, 0.8973, 0.8469, 0.0284],
        [0.8242, 0.2133, 0.4728, 0.7412, 0.5940, 0.6945],
        [0.4149, 0.8437, 0.8417, 0.1371, 0.5830, 0.1980]])
tensor([[0.2205, 0.0761, 0.4101],
        [0.8242, 0.2133, 0.4728],
        [0.4149, 0.8437, 0.8417]])
tensor([[0.8973, 0.8469, 0.0284],
        [0.7412, 0.5940, 0.6945],
        [0.1371, 0.5830, 0.1980]])
~~~

+ torch <=> numpy [텐서를 배열로 변환]

~~~
a = torch.ones(7)
b = a.numpy() #numpy형태로 변환
~~~

+ torch <=> numpy [배열을 텐서로 변환]
~~~
a = np.ones(7)
b = torch.from_numpy(a) #torch형태로 변환
~~~

+ CUDA Tensors => .to 메소드를 사용하여 텐서를 어떠한 장치로도 옮길 수 있음.



123